<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">menu</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="publications.html" class="current">Publication</a></div>
<div class="menu-item"><a href="award.html">Awards</a></div>
<div class="menu-item"><a href="activities.html">Activities</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
</td>
<td id="layout-content">
<p><br /></p>
<h2>publications<br /></h2>
<p>here is a full list of my publications:</p>
<h2>Journals</h2>
<ul>
<li><p><b>Recent Advances of Foundation Language Models-based Continual Learning: A Survey</b><br />
Yutao Yang, <b>Jie Zhou</b>, Xuanwen Ding, Tianyu Huai, Shunyu Liu, Qin Chen, Liang He, Yuan Xie <br />
ACM Computing Survey 2024, <span style="color:red"><i>IF 23.8</i></span>. [<a href="https://arxiv.org/abs/2405.18653">pdf</a>] [<a href="https://github.com/ECNU-ICALK/Foundation-LMs-based-Continual-Learning">Github</a>] <br /></p>
</li>
</ul>
<h2>Conferences</h2>
<p><b>2024</b></p>
<ul>
<li><p><b>Boosting Large Language Models with Continual Learning for Aspect-based Sentiment Analysis</b><br />
Xuanwen Ding, <b>Jie Zhou</b>, Liang Dou, Qin Chen, Yuanbin Wu, Chengcai Chen, Liang He <br />
EMNLP 2024. <br /></p>
</li>
</ul>
<ul>
<li><p><b>From Individual to Society: A Survey on Social Simulation Driven by Large Language Model-based Agents</b><br />
Xinyi Mou, Xuanwen Ding, Qi He, Liang Wang, Jingcong Liang, Xinnong Zhang, Libo Sun, Jiayu Lin, <b>Jie Zhou</b>, Xuanjing Huang, Zhongyu Wei <br />
Arxiv 2024. [<a href="https://arxiv.org/pdf/2412.03563">pdf</a>] <br /></p>
</li>
</ul>
<ul>
<li><p><b>PI-Fed: Continual Federated Learning With Parameter-Level Importance Aggregation</b><br />
Lang Yu, Lina Ge, Guanghui Wang, Jianghao Yin, Qin Chen, <b>Jie Zhou</b>, Liang He <br />
IEEE Internet of Things Joural 2024. <br /></p>
</li>
</ul>
<ul>
<li><p><b>Recent Advances in Robot Navigation via Large Language Models: A Review</b><br />
Haotian Pan, Shibo Huang, Jian Yang, Jinpeng Mi, Ke Li, Xiong You, Xuan Tang, Peidong Liang, Jinbo Yang, Yingjie Liu, Jianfeng Zhang, Muyu Wang, Jie Yang, Xinyu Zhang, Lijun Zhao, Mingsong Chen, <b>Jie Zhou</b>, Xian Wei <br />
Arxiv 2024. <br /></p>
</li>
</ul>
<ul>
<li><p><b>Mathematical language models: A survey</b><br />
Wentao Liu, Hanglei Hu, <b>Jie Zhou</b>, Yuyang Ding, Junsong Li, Jiayi Zeng, Mengliang He, Qin Chen, Bo Jiang, Aimin Zhou, Liang He <br />
Arxiv 2023. </p>
</li>
</ul>
<ul>
<li><p><b>CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the Mathematics Reasoning of Large Multimodal Models</b><br />
Wentao Liu, Qianjun Pan, Yi Zhang, Zhuo Liu, Ji Wu, <b>Jie Zhou</b>, Aimin Zhou, Qin Chen, Bo Jiang, Liang He <br />
Arxiv 2024. [<a href="https://arxiv.org/abs/2409.02834">pdf</a>] [<a href="https://github.com/ECNU-ICALK/EduChat-Math">Github</a>] <br /></p>
</li>
</ul>
<ul>
<li><p><b>Boosting Large Language Models with Socratic Method for Conversational Mathematics Teaching</b><br />
Yuyang Ding, Hanglei Hu, <b>Jie Zhou</b>, Qin Chen, Bo Jiang, Liang He <br />
CIKM 2024. <br /></p>
</li>
<li><p><b>Boosting conversational question answering with fine-grained retrieval-augmentation and self-check</b><br />
Linhao Ye, Zhikai Lei, Jianghao Yin, Qin Chen, <b>Jie Zhou</b>, Liang He
 <br />
SIGIR 2024.  <br /></p>
</li>
<li><p><b>Melo: Enhancing model editing with neuron-indexed dynamic lora</b><br />
Lang Yu, Qin Chen, <b>Jie Zhou</b>, Liang He <br />
AAAI 2024.  <br /></p>
</li>
</ul>
<ul>
<li><p><b>DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models</b><br />
Kedi Chen, Qin Chen, <b>Jie Zhou</b>, Yishen He, Liang He<br />
EMNLP 2024. [<a href="https://arxiv.org/abs/2403.00896">pdf</a>] [<a href="https://github.com/ECNU-ICALK/DiaHalu">Github</a>] </p>
</li>
</ul>
<p><b>2022</b></p>
<ul>
<li><p><b>A Multi-Format Transfer Learning Model for Event Argument Extraction via Variational Information Bottleneck</b><br />
<b>Jie Zhou</b>, Qi Zhang, Qin Chen, Zhang Qi, Liang He, Xuanjing Huang
 <br /> 
COLING 2022, <span style="color:red"><i>Outstanding Paper</i></span> <br /></p>
</li>
</ul>
</td>
</tr>
</table>
</body>
</html>
